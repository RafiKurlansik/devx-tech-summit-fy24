{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13169776-b01f-4dd6-bdc2-c84098bbfe95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Databricks SDK for Python\n",
    "\n",
    "---\n",
    "#### Learning objectives\n",
    "* Authentication with the SDK\n",
    "* How paginated responses and long running operations work\n",
    "* How to create and destroy Workspace resources like clusters and jobs\n",
    "\n",
    "---\n",
    "\n",
    "Picture this: You're at the helm of a turbocharged sports car, and the open road ahead represents the vast and exciting realm of Databricks Lakehouse. The Databricks SDK for Python? Well, think of it as your supercharged engine, the cutting-edge tech that transforms you into the speedster of data development!\n",
    "\n",
    "![gif](https://databricks-sdk-py.readthedocs.io/en/latest/_images/notebook-native-auth.gif)\n",
    "\n",
    "The Databricks SDK is like having a nitro boost for Python development in the Databricks Lakehouse. It's your secret sauce to turbocharge your coding abilities. Just like a sports car can handle all kinds of terrain, this SDK covers all public Databricks REST API operations. It's your all-access pass to the entire spectrum of Databricks features and functionalities. Imagine having an autopilot system that can handle unexpected twists and turns effortlessly. The SDK's internal HTTP client is as robust as your sports car's suspension, ensuring smooth handling even when things get bumpy. It's got your back with intelligent retries, so you can keep moving forward without a hiccup. Much like the precision engineering behind a high-performance vehicle, this SDK is meticulously crafted to provide you with the utmost control and accuracy over your Databricks projects. This SDK isn't just fast; it's fuel-efficient too. It streamlines your development process, making it sleek and efficient, so you can get more done in less time. You can always find the latest documentation at [https://databricks-sdk-py.readthedocs.io](https://databricks-sdk-py.readthedocs.io/en/latest/)\n",
    "\n",
    "Whether you're scripting like a code ninja in the shell, orchestrating seamless CI/CD production setups, or conducting symphonies of data from the Databricks Notebook, this SDK is your all-in-one, full-throttle, programmatic powerhouse. For now, Databricks Runtime may have outdated versions of Python SDK, so until further notice, make sure to always install the latest version `%pip install databricks-sdk==0.8.0` within a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde41484-a5dd-4e5d-b542-2d37e1b1e4b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk==0.8.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "468d4065-9544-4339-94ee-da36835c2667",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Once you've installed it, prepare to witness the magic of simplicity and security! Initializing the WorkspaceClient with the Databricks SDK for Python is like waving a wand that effortlessly picks up authentication from the notebook context. Say goodbye to the brittle hassle of passing tokens around. Thanks to the Unified Client Authentication, it's as if you've cast a spell of harmony across your Databricks tools and integrations. Whether you're running the Databricks Terraform Provider, harnessing the Databricks SDK for Go, wielding the Databricks CLI, or even working with applications targeting Databricks SDKs in other languages, they'll all play together in perfect harmony. It's like a symphony of data tools working seamlessly to empower your data-driven dreams! Read more about Python SDK with Azure CLI, Azure Service Principals, Databricks Account-level Service Principals, and Databricks CLI authentication [here](https://databricks-sdk-py.readthedocs.io/en/latest/authentication.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ed08825-1b97-427c-a1fd-bd363ba1c677",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "w = WorkspaceClient()\n",
    "me = w.current_user.me()\n",
    "print(f'My email is {me.user_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "050a50e5-5736-4196-bed2-35ed5a5e1510",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Paginated Responses\n",
    "In the dynamic world of Databricks APIs, dealing with different pagination styles used to be like solving a complex puzzle. But here's where the magic of our SDKs truly shines. Whether you're wielding the power of Go, Python, Java, R, the Databricks CLI, or any other SDK in our arsenal, they all possess an extraordinary superpower: transparently handling paginated responses! It's like having a universal translator for APIs. Regardless of whether an API uses offset-plus-limit, starts offsets from 0 or 1, employs cursor-based iteration, or simply returns everything in one massive response, our SDKs provide you with a consistent list method that seamlessly returns an **iterator**. It's the ultimate weapon in your arsenal, ensuring that you can conquer any API pagination challenge with ease and grace, making you the undisputed champion of data manipulation and exploration!\n",
    "\n",
    "In the following example we'll just list all of the jobs we have in the workspace, which obviously spans across multiple pages of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f8dc622-c5ab-4370-9424-1e89780bd3f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for job in w.jobs.list():\n",
    "    print(f'Found job: {job.settings.name} (id: {job.job_id})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71c072a7-8f91-4a3b-bce9-0813cf312080",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Want something more personal? Let's look at own notebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fc92fc-8bd8-4d19-81ae-835f1faed6f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for obj in w.workspace.list(f'/Users/{w.current_user.me().user_name}', recursive=True):\n",
    "    print(f'{obj.object_type}: {obj.path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8258b444-1c73-407c-9aa6-b564f0e8472c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's cook up a script that's like your very own mission control center, monitoring the pulse of your jobs with style. Imagine a dashboard that not only reveals the latest execution status of each job but also calculates the average run duration for each one. And the best part? We're going full-throttle excitement by putting the most recent updates right at the top, so you're always in the loop with the freshest data insights. This script is your ticket to a dynamic world of job monitoring that's as thrilling as it is efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5b67fd-a569-4b5e-b630-3cbe1b04eb2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from collections import defaultdict \n",
    "from datetime import datetime, timezone \n",
    "from databricks.sdk.service.jobs import PauseStatus \n",
    "\n",
    "latest_state = {} # Initialize an empty dictionary for storing the latest state of jobs\n",
    "all_jobs = {} # Initialize an empty dictionary for storing all databricks jobs\n",
    "durations = defaultdict(list) # Initialize a defaultdict object for easily creating lists\n",
    "\n",
    "# Loop through all jobs in databricks workspace\n",
    "for job in w.jobs.list():\n",
    "    all_jobs[job.job_id] = job # Append the current job to the all_jobs dictionary\n",
    "    if job.settings.schedule is None: # Skip jobs that are not scheduled\n",
    "        continue \n",
    "    for run in w.jobs.list_runs(job_id=job.job_id, expand_tasks=False): # Loop through all runs of the current job\n",
    "        if run.run_duration is not None: # Capture how long the job ran\n",
    "            durations[job.job_id].append(run.run_duration)\n",
    "        if job.job_id not in latest_state: # Capture the latest state of a job if it is not already captured\n",
    "            latest_state[job.job_id] = run\n",
    "            continue\n",
    "        if run.end_time < latest_state[job.job_id].end_time: # Skip run that is older than the existing record\n",
    "            continue\n",
    "        latest_state[job.job_id] = run\n",
    "\n",
    "summary = [] # Initialize an empty list for storing the summary of job statuses\n",
    "for job_id, run in latest_state.items(): # Loop through previously captured latest states of all jobs\n",
    "    average_duration = 0\n",
    "    if len(durations[job_id]) > 0:\n",
    "        average_duration = sum(durations[job_id]) / len(durations[job_id])\n",
    "    summary.append({\n",
    "        # Append to the summary the job's name, last status (success or failure), last finished time, and average run duration\n",
    "        'job_name': all_jobs[job_id].settings.name,\n",
    "        'last_status': run.state.result_state,\n",
    "        'last_finished': datetime.fromtimestamp(run.end_time/1000, timezone.utc),\n",
    "        'average_duration': average_duration\n",
    "    })\n",
    "\n",
    "for line in sorted(summary, key=lambda s: s['last_finished'], reverse=True):\n",
    "    # Print the summary of all jobs' statuses, sorted by date of last finish, in reversed chronological order\n",
    "    print(f'Latest: {line}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7ebd83d-877f-47f2-aa34-f934fd516711",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Long-running Operations\n",
    "\n",
    "When you invoke a long-running operation, the SDK provides a high-level API to _trigger_ these operations and _wait_ for the related entities to reach the correct state or return the error message in case of failure. All long-running operations return generic `Wait` instance with `result()` method to get a result of long-running operation, once it's finished. Databricks SDK for Python picks the most reasonable default timeouts for every method, but sometimes you may find yourself in a situation, where you'd want to provide `datetime.timedelta()` as the value of `timeout`\n",
    "argument to `result()` method.\n",
    "\n",
    "There are a number of long-running operations in Databricks APIs such as managing:\n",
    "* Clusters,\n",
    "* Command execution\n",
    "* Jobs\n",
    "* Libraries\n",
    "* Delta Live Tables pipelines\n",
    "* Databricks SQL warehouses.\n",
    "\n",
    "For example, in the Clusters API, once you create a cluster, you receive a cluster ID, and the cluster is in the `PENDING` state Meanwhile\n",
    "Databricks takes care of provisioning virtual machines from the cloud provider in the background. The cluster is\n",
    "only usable in the `RUNNING` state and so you have to wait for that state to be reached.\n",
    "\n",
    "Another example is the API for running a job or repairing the run: right after\n",
    "the run starts, the run is in the `PENDING` state. The job is only considered to be finished when it is in either\n",
    "the `TERMINATED` or `SKIPPED` state. Also you would likely need the error message if the long-running\n",
    "operation times out and fails with an error code. Other times you may want to configure a custom timeout other than\n",
    "the default of 20 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6acf695-b3ea-4e72-a7d6-6bc94c8de236",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from databricks.sdk.service.compute import DataSecurityMode, RuntimeEngine\n",
    "\n",
    "# This will use the DBAcademy cluster policy, which we need for the CloudLabs environment\n",
    "info = w.clusters.create(cluster_name=f'Created cluster from {w.current_user.me().user_name}',\n",
    "                         spark_version=w.clusters.select_spark_version(latest=True),\n",
    "                         autotermination_minutes=10,\n",
    "                         runtime_engine=RuntimeEngine.STANDARD,\n",
    "                         data_security_mode=DataSecurityMode.SINGLE_USER,\n",
    "                         policy_id=[p for p in w.cluster_policies.list() if p.name == \"DBAcademy\"][0].policy_id).result(timeout=datetime.timedelta(minutes=10))\n",
    "print(f'Created: {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b210588c-6f25-4cea-bc52-6cdefd6d8715",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Example workload\n",
    "\n",
    "Let's create a notebook in our home directory that shows names of all databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce72a7e7-2377-4f4a-9fa0-3a47da53bd53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_path = f'/Users/{w.current_user.me().user_name}/sdk-sample.py'\n",
    "notebook_content = 'display(spark.sql(\"SHOW DATABASES\"))'\n",
    "w.workspace.upload(notebook_path, notebook_content.encode('utf8'), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52607d1c-7b66-4e19-be0b-56401ba2e7c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "and then create a job that runs this notebook on the interactive cluster we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d2909a-a73e-42f8-aa27-e7492444c8a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.jobs import Task, NotebookTask\n",
    "from databricks.sdk.service.compute import ClusterSpec, DataSecurityMode, RuntimeEngine\n",
    "\n",
    "latest_runtime = w.clusters.select_spark_version(latest=True)\n",
    "smallest_node = w.clusters.select_node_type(local_disk=True)\n",
    "\n",
    "job = w.jobs.create(name=f'Job created from SDK by {w.current_user.me().user_name}',\n",
    "                    tasks=[Task(task_key='main',\n",
    "                                notebook_task=NotebookTask(notebook_path),\n",
    "                                existing_cluster_id=info.cluster_id)])\n",
    "print(f'Created job with id: {job.job_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89bbf551-5b87-4bb9-a4eb-e8e7b229ac8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "and then start this job and wait until it's completed or timeout within 10 minutes. It's always a good practice to set a client-side timeout for long-running operations. \n",
    "\n",
    "Please open [jobs owned by me](/#job/list?acl=owned_by_me) after running the cell below and confirm it's running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17182b0-bde5-4b01-996e-2966ce2f10e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "w.jobs.run_now(job.job_id).result(timeout=datetime.timedelta(minutes=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0db6ea-1d07-475f-a144-68ef46966a27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    ".. and don't forget to remove the notebook, job, and cluster you've just created after you've done experimenting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9d6714-f964-4471-8cf0-a9a806445a72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w.workspace.delete(notebook_path)\n",
    "w.jobs.delete(job.job_id)\n",
    "w.clusters.permanent_delete(info.cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "985c0ca1-aa45-4c79-b6d4-4440f334d689",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Debug Logging\n",
    "\n",
    "The Databricks SDK for Python seamlessly integrates with the standard Logging facility for Python. This allows developers to easily enable and customize logging for their Databricks Python projects. To enable debug logging in your Databricks Python project, you can execute the snippet below and then re-run any cell that calls the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44bb1a80-78d3-4784-bae2-1f90fc812c11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "logging.basicConfig(stream=sys.stderr,\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s [%(name)s][%(levelname)s] %(message)s')\n",
    "logging.getLogger('databricks.sdk').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff4602d3-6548-4563-be9b-e6eadbfa8c2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## More Code Examples\n",
    "\n",
    "Please checkout [OAuth with Flask](https://github.com/databricks/databricks-sdk-py/tree/main/examples/flask_app_with_oauth.py), \n",
    "[Last job runs](https://github.com/databricks/databricks-sdk-py/tree/main/examples/last_job_runs.py), \n",
    "[Starting job and waiting](https://github.com/databricks/databricks-sdk-py/tree/main/examples/starting_job_and_waiting.py) examples. You can also dig deeper into different services, like\n",
    "[alerts](https://github.com/databricks/databricks-sdk-py/tree/main/examples/alerts), \n",
    "[billable_usage](https://github.com/databricks/databricks-sdk-py/tree/main/examples/billable_usage), \n",
    "[catalogs](https://github.com/databricks/databricks-sdk-py/tree/main/examples/catalogs), \n",
    "[cluster_policies](https://github.com/databricks/databricks-sdk-py/tree/main/examples/cluster_policies), \n",
    "[clusters](https://github.com/databricks/databricks-sdk-py/tree/main/examples/clusters), \n",
    "[credentials](https://github.com/databricks/databricks-sdk-py/tree/main/examples/credentials), \n",
    "[current_user](https://github.com/databricks/databricks-sdk-py/tree/main/examples/current_user), \n",
    "[dashboards](https://github.com/databricks/databricks-sdk-py/tree/main/examples/dashboards), \n",
    "[data_sources](https://github.com/databricks/databricks-sdk-py/tree/main/examples/data_sources), \n",
    "[databricks](https://github.com/databricks/databricks-sdk-py/tree/main/examples/databricks), \n",
    "[encryption_keys](https://github.com/databricks/databricks-sdk-py/tree/main/examples/encryption_keys), \n",
    "[experiments](https://github.com/databricks/databricks-sdk-py/tree/main/examples/experiments), \n",
    "[external_locations](https://github.com/databricks/databricks-sdk-py/tree/main/examples/external_locations), \n",
    "[git_credentials](https://github.com/databricks/databricks-sdk-py/tree/main/examples/git_credentials), \n",
    "[global_init_scripts](https://github.com/databricks/databricks-sdk-py/tree/main/examples/global_init_scripts), \n",
    "[groups](https://github.com/databricks/databricks-sdk-py/tree/main/examples/groups), \n",
    "[instance_pools](https://github.com/databricks/databricks-sdk-py/tree/main/examples/instance_pools), \n",
    "[instance_profiles](https://github.com/databricks/databricks-sdk-py/tree/main/examples/instance_profiles), \n",
    "[ip_access_lists](https://github.com/databricks/databricks-sdk-py/tree/main/examples/ip_access_lists), \n",
    "[jobs](https://github.com/databricks/databricks-sdk-py/tree/main/examples/jobs), \n",
    "[libraries](https://github.com/databricks/databricks-sdk-py/tree/main/examples/libraries), \n",
    "[local_browser_oauth.py](https://github.com/databricks/databricks-sdk-py/tree/main/examples/local_browser_oauth.py), \n",
    "[log_delivery](https://github.com/databricks/databricks-sdk-py/tree/main/examples/log_delivery), \n",
    "[metastores](https://github.com/databricks/databricks-sdk-py/tree/main/examples/metastores), \n",
    "[model_registry](https://github.com/databricks/databricks-sdk-py/tree/main/examples/model_registry), \n",
    "[networks](https://github.com/databricks/databricks-sdk-py/tree/main/examples/networks), \n",
    "[permissions](https://github.com/databricks/databricks-sdk-py/tree/main/examples/permissions), \n",
    "[pipelines](https://github.com/databricks/databricks-sdk-py/tree/main/examples/pipelines), \n",
    "[private_access](https://github.com/databricks/databricks-sdk-py/tree/main/examples/private_access), \n",
    "[queries](https://github.com/databricks/databricks-sdk-py/tree/main/examples/queries), \n",
    "[recipients](https://github.com/databricks/databricks-sdk-py/tree/main/examples/recipients), \n",
    "[repos](https://github.com/databricks/databricks-sdk-py/tree/main/examples/repos), \n",
    "[schemas](https://github.com/databricks/databricks-sdk-py/tree/main/examples/schemas), \n",
    "[secrets](https://github.com/databricks/databricks-sdk-py/tree/main/examples/secrets), \n",
    "[service_principals](https://github.com/databricks/databricks-sdk-py/tree/main/examples/service_principals), \n",
    "[storage](https://github.com/databricks/databricks-sdk-py/tree/main/examples/storage), \n",
    "[storage_credentials](https://github.com/databricks/databricks-sdk-py/tree/main/examples/storage_credentials), \n",
    "[tokens](https://github.com/databricks/databricks-sdk-py/tree/main/examples/tokens), \n",
    "[users](https://github.com/databricks/databricks-sdk-py/tree/main/examples/users), \n",
    "[vpc_endpoints](https://github.com/databricks/databricks-sdk-py/tree/main/examples/vpc_endpoints), \n",
    "[warehouses](https://github.com/databricks/databricks-sdk-py/tree/main/examples/warehouses), \n",
    "[workspace](https://github.com/databricks/databricks-sdk-py/tree/main/examples/workspace), \n",
    "[workspace_assignment](https://github.com/databricks/databricks-sdk-py/tree/main/examples/workspace_assignment), \n",
    "[workspace_conf](https://github.com/databricks/databricks-sdk-py/tree/main/examples/workspace_conf), \n",
    "and [workspaces](https://github.com/databricks/databricks-sdk-py/tree/main/examples/workspaces).\n",
    "\n",
    "And, of course, https://databricks-sdk-py.readthedocs.io/."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "databricks_sdk_for_python",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
